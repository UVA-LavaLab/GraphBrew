[![Build Status](https://app.travis-ci.com/UVA-LavaLab/GraphBrew.svg?branch=main)](https://app.travis-ci.com/UVA-LavaLab/GraphBrew)
[![Wiki](https://img.shields.io/badge/ðŸ“š_Wiki-Documentation-blue?style=flat)](https://github.com/UVA-LavaLab/GraphBrew/wiki)

[<p align="center"><img src="./docs/figures/logo.svg" width="200" ></p>](#graphbrew)

# GraphBrew <img src="./docs/figures/logo_left.png" width="50" align="center">

A graph reordering and benchmarking framework built on the [GAP Benchmark Suite (GAPBS)](https://github.com/sbeamer/gapbs). GraphBrew reorders graph vertices to improve cache locality and speed up graph algorithms â€” with **16 algorithm IDs** (12 reorderers + 2 baselines + 2 meta), an **ML-based adaptive selector**, and a **one-click experiment pipeline**.

> **ðŸ“– Full documentation:** [Wiki](https://github.com/UVA-LavaLab/GraphBrew/wiki) Â· [Quick Start](https://github.com/UVA-LavaLab/GraphBrew/wiki/Quick-Start) Â· [Command-Line Reference](https://github.com/UVA-LavaLab/GraphBrew/wiki/Command-Line-Reference)

---

## Quick Start

```bash
# Clone and build
git clone https://github.com/UVA-LavaLab/GraphBrew.git
cd GraphBrew
make all

# Run PageRank with AdaptiveOrder (ML-selected best algorithm)
./bench/bin/pr -g 20 -o 14

# Run BFS with GraphBrewOrder (community-based reordering)
./bench/bin/bfs -f graph.mtx -o 12
```

### Build

RabbitOrder is enabled by default and requires Boost, libnuma, and google-perftools.
See [Installation Wiki](https://github.com/UVA-LavaLab/GraphBrew/wiki/Installation) for details.

```bash
# Default build (RabbitOrder enabled)
make all

# Build without RabbitOrder (no Boost/libnuma/tcmalloc needed)
RABBIT_ENABLE=0 make all
```

---

## Reordering Algorithms

GraphBrew provides 16 algorithm IDs. IDs 0-1 are baselines (graph states), IDs 2-12 and 15 produce reorderings, and IDs 13-14 are meta-algorithms. Use `-o <id>` to select one:

| ID | Algorithm | Description |
|----|-----------|-------------|
| 0 | ORIGINAL | No reordering (baseline) |
| 1 | RANDOM | Random permutation (baseline) |
| 2 | SORT | Sort by degree |
| 3 | HUBSORT | Hub-based sorting |
| 4 | HUBCLUSTER | Hub-score clustering |
| 5 | DBG | Degree-based grouping |
| 6 | HUBSORTDBG | HubSort + DBG |
| 7 | HUBCLUSTERDBG | HubCluster + DBG |
| 8 | RABBITORDER | Community clustering (variants: `csr` / `boost`) |
| 9 | GORDER | Window-based cache optimization (variants: `default` / `csr` / `fast`) |
| 10 | CORDER | Workload-balanced reordering |
| 11 | RCM | Reverse Cuthill-McKee |
| 12 | GRAPHBREWORDER | Leiden clustering + configurable per-community order |
| 13 | MAP | Load ordering from file (`-o 13:mapping.lo`) |
| 14 | **ADAPTIVEORDER** | ML perceptron â€” automatically picks the best algorithm â­ |
| 15 | LEIDENORDER | Leiden via GVE-Leiden library (`15:resolution`) â€” baseline reference |

### Which Algorithm Should I Use?

| Graph Type | Recommended | Why |
|------------|-------------|-----|
| Social networks | `12` or `14` | Best community detection + cache locality |
| Web graphs | `12:hrab` or `12` | Hybrid Leiden+Rabbit for best locality |
| Road networks | `11` or `9` | BFS-based approaches for sparse graphs |
| Unknown / mixed | `14` | Let the ML perceptron decide |

---

## One-Click Experiment Pipeline

Run the complete benchmark + training workflow with one command:

```bash
# Install Python dependencies
pip install -r scripts/requirements.txt

# Full pipeline: download graphs â†’ build â†’ benchmark â†’ store results for C++ runtime ML
python3 scripts/graphbrew_experiment.py --train --all-variants --size medium --auto --trials 5
```

| Parameter | Description |
|-----------|-------------|
| `--train` | Run the complete pipeline (benchmark + store results for C++ runtime training) |
| `--full` | Run full evaluation pipeline (no training) |
| `--all-variants` | Test all algorithm variants |
| `--size SIZE` | Graph category: `small` (62 MB), `medium` (1.1 GB), `large` (25 GB), `xlarge` (63 GB), `all` (89 GB) |
| `--auto` | Auto-detect RAM/disk limits |
| `--trials N` | Benchmark trials (default: 2) |
| `--quick` | Test only key algorithms (faster) |
| `--brute-force` | Compare adaptive selection vs all eligible algorithms |
| `--download-only` | Download graphs without running benchmarks |

Results are saved to `./results/`. Benchmark data goes to `./results/data/` â€” C++ trains ML models (perceptron, DT, hybrid) at runtime from this data.

```bash
# See all options
python3 scripts/graphbrew_experiment.py --help
```

> ðŸ“– See [Running Benchmarks Wiki](https://github.com/UVA-LavaLab/GraphBrew/wiki/Running-Benchmarks) for advanced workflows.

---

## Graph Benchmarks

Built on [GAPBS](https://github.com/sbeamer/gapbs), GraphBrew includes 8 benchmarks. The experiment pipeline defaults to 7 (`EXPERIMENT_BENCHMARKS`) â€” TC is excluded because triangle counting is a combinatorial kernel that doesn't benefit from vertex reordering.

| Benchmark | Algorithm | In Experiments |
|-----------|-----------|:-:|
| `pr` | PageRank | âœ“ |
| `pr_spmv` | PageRank (SpMV variant) | âœ“ |
| `bfs` | Breadth-First Search (direction optimized) | âœ“ |
| `cc` | Connected Components (Afforest) | âœ“ |
| `cc_sv` | Connected Components (Shiloach-Vishkin) | âœ“ |
| `sssp` | Single-Source Shortest Paths | âœ“ |
| `bc` | Betweenness Centrality | âœ“ |
| `tc` | Triangle Counting | â€” |

> **Random Baseline:** By default, graphs are converted to `.sg` with RANDOM vertex ordering so all benchmark measurements reflect improvement over a worst-case baseline. Use `--no-random-baseline` to disable.

```bash
# Run a single benchmark
make run-bfs

# Run with parameters
./bench/bin/pr -f graph.mtx -n 16 -o 12

# Generate a reordered graph
./bench/bin/converter -f graph.mtx -p reordered.mtx -o 12
```

> ðŸ“– See [Graph Benchmarks Wiki](https://github.com/UVA-LavaLab/GraphBrew/wiki/Graph-Benchmarks) for details.

---

## Supported Graph Formats

GraphBrew can load graphs in these formats:

| Extension | Format |
|-----------|--------|
| `.el` | Edge list (node1 node2) |
| `.wel` | Weighted edge list |
| `.mtx` | Matrix Market |
| `.gr` | DIMACS |
| `.graph` | Metis |
| `.sg` / `.wsg` | Serialized (pre-built via `converter`) |

Graphs can also be generated synthetically:
- `-g 20` â€” Kronecker graph with 2Â²â° vertices (Graph500)
- `-u 20` â€” Uniform random graph with 2Â²â° vertices

> ðŸ“– See [Supported Graph Formats Wiki](https://github.com/UVA-LavaLab/GraphBrew/wiki/Supported-Graph-Formats) for details.

---

## Prerequisites

- **OS:** Ubuntu 22.04+ (or any Linux with GCC 7+)
- **Compiler:** `g++` with C++17 and OpenMP support (GCC 9+ preferred)
- **Build:** `make`
- **Python:** 3.8+ (for experiment scripts)

RabbitOrder (enabled by default): Boost 1.58+, libnuma, google-perftools.
Use `RABBIT_ENABLE=0 make all` to build without these dependencies.

> ðŸ“– See [Installation Wiki](https://github.com/UVA-LavaLab/GraphBrew/wiki/Installation) for full setup instructions including Boost.

---

## Project Layout

```
bench/
â”œâ”€â”€ src/          # Benchmark source files (bc.cc, bfs.cc, pr.cc, ...)
â”œâ”€â”€ src_sim/      # Cache simulation variants
â”œâ”€â”€ bin/          # Compiled binaries
â””â”€â”€ include/
    â”œâ”€â”€ graphbrew/    # Reordering algorithms & partitioning
    â”œâ”€â”€ external/     # Bundled libraries (GAPBS, RabbitOrder, GOrder, COrder, Leiden)
    â””â”€â”€ cache_sim/    # Cache simulation headers

scripts/
â”œâ”€â”€ graphbrew_experiment.py   # Main experiment orchestration
â”œâ”€â”€ lib/                      # Core Python modules
â””â”€â”€ test/                     # Test suite

results/                      # Benchmark outputs, graph features, mappings
```

> ðŸ“– See [Code Architecture Wiki](https://github.com/UVA-LavaLab/GraphBrew/wiki/Code-Architecture) for the full layout.

---

## Testing

```bash
pip install -r scripts/requirements.txt
pytest scripts/test -q

# Topology verification (ensures reordering preserves graph structure)
make test-topology
```

---

## Developer Tooling

```bash
make lint-includes              # Check for legacy include paths
make help                       # Show all Make targets
make help-pr                    # Show parameters for a specific benchmark
```

---

## How to Cite

If you use GraphBrew in your research, please cite:

- S. Beamer, K. AsanoviÄ‡, D. Patterson, "The GAP Benchmark Suite," arXiv:1508.03619, 2017.
- J. Arai, H. Shiokawa, T. Yamamuro, M. Onizuka, S. Iwamura, "Rabbit Order: Just-in-time Parallel Reordering for Fast Graph Analysis."
- P. Faldu, J. Diamond, B. Grot, "A Closer Look at Lightweight Graph Reordering," arXiv:2001.08448, 2020.
- S. Sahu, "GVE-Leiden: Fast Leiden Algorithm for Community Detection in Shared Memory Setting," arXiv:2312.13936, 2024.
- V. A. Traag, L. Waltman, N. J. van Eck, "From Louvain to Leiden: guaranteeing well-connected communities," Sci Rep 9, 5233, 2019.
- H. Wei, J. X. Yu, C. Lu, X. Lin, "Speedup Graph Processing by Graph Ordering," SIGMOD 2016.
- Y. Chen, Y.-C. Chung, "Workload Balancing via Graph Reordering on Multicore Systems," IEEE TPDS, 2021.

---

## License

See [LICENSE](LICENSE) for details.
