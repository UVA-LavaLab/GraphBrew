# Python Scripts Guide

Documentation for all Python tools in the GraphBrew framework.

## Overview

The scripts folder contains a single entry point and a modular library (`lib/`):

```
scripts/
‚îú‚îÄ‚îÄ graphbrew_experiment.py      # ‚≠ê MAIN: Single entry point for all experiments
‚îú‚îÄ‚îÄ requirements.txt             # Python dependencies
‚îÇ
‚îú‚îÄ‚îÄ lib/                         # üì¶ Modular library (5 sub-packages)
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py              # Re-exports every public name (backward-compatible)
‚îÇ   ‚îú‚îÄ‚îÄ README.md                # Package map & import conventions
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ core/                    # Constants, logging, data stores
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ utils.py             # SSOT ‚Äî algorithm IDs, variant registry, paths, logging
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ graph_types.py       # Data classes (GraphInfo, BenchmarkResult, etc.)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ datastore.py         # BenchmarkStore, GraphPropsStore (append-only JSON DBs)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ graph_data.py        # Graph metadata & dataset catalog
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ pipeline/                # Experiment execution stages
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dependencies.py      # System dependency detection & installation
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ build.py             # Binary compilation utilities
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ download.py          # Graph downloading from SuiteSparse
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ reorder.py           # Vertex reordering generation
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ benchmark.py         # Performance benchmark execution
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cache.py             # Cache simulation analysis
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ phases.py            # Phase orchestration (reorder ‚Üí benchmark ‚Üí cache)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ progress.py          # Progress tracking & reporting
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ ml/                      # ML scoring & training (legacy / fallback)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ weights.py           # SSO scoring ‚Äî PerceptronWeight.compute_score()
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ eval_weights.py      # Weight evaluation & data loading
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ training.py          # Iterative / batched perceptron training
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ adaptive_emulator.py # C++ AdaptiveOrder logic emulation
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ oracle.py            # Oracle (best-possible) analysis
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ features.py          # Graph topology feature extraction
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ analysis/                # Post-run analysis & visualisation
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ adaptive.py          # A/B testing, Leiden variant comparison
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ metrics.py           # Amortization & end-to-end metrics
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ figures.py           # SVG / PNG plot generation
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ tools/                   # Standalone CLI utilities
‚îÇ       ‚îú‚îÄ‚îÄ check_includes.py    # C++ header-include linting
‚îÇ       ‚îî‚îÄ‚îÄ regen_features.py    # Feature-vector regeneration
‚îÇ
‚îú‚îÄ‚îÄ test/                        # Pytest suite
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ test_algorithm_variants.py
‚îÇ   ‚îú‚îÄ‚îÄ test_cache_simulation.py
‚îÇ   ‚îú‚îÄ‚îÄ test_experiment_validation.py
‚îÇ   ‚îú‚îÄ‚îÄ test_fill_adaptive.py
‚îÇ   ‚îú‚îÄ‚îÄ test_fill_weights_variants.py
‚îÇ   ‚îú‚îÄ‚îÄ test_graphbrew_experiment.py
‚îÇ   ‚îú‚îÄ‚îÄ test_multilayer_validity.py
‚îÇ   ‚îú‚îÄ‚îÄ test_self_recording.py
‚îÇ   ‚îú‚îÄ‚îÄ test_weight_flow.py
‚îÇ   ‚îú‚îÄ‚îÄ data/                    # Test data fixtures
‚îÇ   ‚îî‚îÄ‚îÄ graphs/                  # Test graph fixtures
‚îÇ
‚îî‚îÄ‚îÄ README.md
```

---

## ‚≠ê graphbrew_experiment.py - Main Orchestration

The main script provides orchestration over the `lib/` modules. It handles argument parsing and calls the appropriate phase functions.

### Quick Start

```bash
# Full pipeline: download ‚Üí build ‚Üí experiment ‚Üí weights
python3 scripts/graphbrew_experiment.py --full --size small

# See all options
python3 scripts/graphbrew_experiment.py --help
```

### Key Features

| Feature | Description |
|---------|-------------|
| **Graph Download** | Downloads from SuiteSparse collection (87 graphs available) |
| **Auto Build** | Compiles binaries if missing |
| **Memory Management** | Automatically skips graphs exceeding RAM limits |
| **Label Maps** | Pre-generates reordering maps for consistency |
| **Reordering** | Tests all 16 algorithm IDs (0-15), with variants (RCM:bnf, RabbitOrder:csr/boost, GOrder:csr/fast, etc.) |
| **Benchmarks** | PR, PR_SPMV, BFS, CC, CC_SV, SSSP, BC, TC |
| **Cache Simulation** | L1/L2/L3 hit rate analysis |
| **Brute-Force Validation** | Compares adaptive vs all algorithms |

---

## üîç Adaptive Emulator (--emulator)

**Pure Python emulator that replicates C++ AdaptiveOrder logic without recompiling.**

This is useful for:
- Analyzing how weight changes affect algorithm selection
- Testing weight configurations quickly in Python
- Understanding the two-layer selection process (type matching + perceptron)
- Debugging why a specific algorithm was chosen

### Quick Start

```bash
# Via entry point
python3 scripts/graphbrew_experiment.py --emulator

# Or directly via module (supports full argparse)
python3 -m scripts.lib.ml.adaptive_emulator --graph graphs/email-Enron/email-Enron.mtx
python3 -m scripts.lib.ml.adaptive_emulator --compare-benchmark results/benchmark_*.json
python3 -m scripts.lib.ml.adaptive_emulator --all-graphs --disable-weight w_modularity
python3 -m scripts.lib.ml.adaptive_emulator --mode best-endtoend --compare-benchmark results/benchmark.json
```

### Selection Modes

| Mode | Description |
|------|-------------|
| `fastest-reorder` | Minimize reordering time only |
| `fastest-execution` | Minimize algorithm execution time (default) |
| `best-endtoend` | Minimize (reorder_time + execution_time) |
| `best-amortization` | Minimize iterations needed to amortize reordering cost |
| `heuristic` | Feature-based heuristic (more robust) |
| `type-bench` | Type+benchmark recommendations (best accuracy) |

### How It Works

The emulator replicates the C++ AdaptiveOrder two-layer selection:

```
Layer 1: Type Matching
  - Compute graph features ‚Üí normalized vector
  - Find closest type centroid (Euclidean distance)
  - OOD check: if distance > 1.5 ‚Üí return ORIGINAL
  - Load that type's weights

Layer 2: Algorithm Selection
  - Compute perceptron scores for each algorithm
  - Score = bias + Œ£(weight_i √ó feature_i)
          + quadratic cross-terms (dv√óhub, mod√ólogN, pf√ówsr)
          + convergence bonus (PR/SSSP only)
  - ORIGINAL margin check: if best - ORIGINAL < 0.05 ‚Üí ORIGINAL
  - Select algorithm with highest score
```

### Emulator vs Perceptron Training

| Tool | Purpose |
|------|---------|  
| `--emulator` / `scripts.lib.ml.adaptive_emulator` | Emulate C++ selection logic, analyze weight impact |
| `--eval-weights` / `scripts.lib.ml.eval_weights` | Evaluate weights, train from benchmark data |

Use **adaptive_emulator** when you want to understand why a specific algorithm was selected.
Use **eval_weights** (via `--eval-weights`) when you want to train or evaluate weights.

---

## üìä Weight Evaluation (--eval-weights)

**Quick evaluation script that trains weights, simulates C++ `scoreBase() √ó benchmarkMultiplier()` scoring, and reports accuracy/regret metrics.**

This is the fastest way to validate that your trained weights actually produce good algorithm selections without recompiling C++ or running live benchmarks.

### Quick Start

```bash
python3 scripts/graphbrew_experiment.py --eval-weights
python3 scripts/graphbrew_experiment.py --eval-weights --sg-only
```

### What It Does

1. **Loads** the latest `benchmark_*.json` and `reorder_*.json` from `results/`
2. **Trains** weights via `compute_weights_from_results()` (multi-restart perceptrons + regret-aware grid search)
3. **Saves** weights to `results/data/adaptive_models.json`
4. **Simulates** C++ scoring: for each (graph, benchmark), computes `scoreBase(algo, features) √ó benchmarkMultiplier(algo, bench)` for all algorithms
5. **Compares** predicted winner vs actual fastest algorithm (base-aware: variants of same algorithm count as correct)
6. **Reports** accuracy, regret, top-2 accuracy, and per-benchmark breakdown

### Output Metrics

| Metric | Description |
|--------|-------------|
| **Accuracy** | % of (graph, benchmark) pairs where predicted base algorithm matches actual best |
| **Top-2 accuracy** | % where prediction is in the top 2 fastest algorithms |
| **Avg regret** | Average (predicted_time ‚àí best_time) / best_time across all predictions |
| **Median regret** | Median of the above (more robust to outliers) |
| **Base-aware regret** | Same as regret, but variant mismatches within the same base = 0% |
| **Per-benchmark accuracy** | Breakdown by pr, pr_spmv, bfs, cc, cc_sv, sssp, bc, tc |

### Example Output

```
=== Simulating C++ adaptive selection ===

Overall accuracy: XX/YY = ZZ.Z%
Unique predicted algorithms: N: [...]

Per-benchmark accuracy:
  bfs:      .../... = ...%
  cc:       .../... = ...%
  pr:       .../... = ...%
  sssp:     .../... = ...%
  bc:       .../... = ...%
  tc:       .../... = ...%

Average regret: X.X% (lower is better)
Top-2 accuracy: .../... = ...%
Median regret: X.X%
Base-aware avg regret: X.X% (variant mismatches = 0%)
Base-aware median regret: X.X%
```

### How C++ Scoring Is Simulated (SSO)

Scoring uses a **single source of truth** ‚Äî the canonical `PerceptronWeight.compute_score()` method in `weights.py`. No duplicated formula:

```python
def _simulate_score(algo_data, feats, benchmark='pr'):
    """Simulate C++ scoreBase() √ó benchmarkMultiplier().

    Delegates to PerceptronWeight.compute_score() ‚Äî SSO scoring.
    Covers all 17 features + 3 quadratic terms + convergence bonus
    + cache constants + benchmark multiplier.
    """
    pw = PerceptronWeight.from_dict(algo_data)
    return pw.compute_score(feats, benchmark)
```

This ensures Python evaluation matches C++ `scoreBase() √ó benchmarkMultiplier()` exactly, with no risk of divergence from a separately maintained formula.

### vs Other Tools

| Tool | Purpose | Updates Weights? |
|------|---------|------------------|
| `eval_weights.py` | Train + evaluate + report accuracy/regret | ‚úÖ Yes |
| `adaptive_emulator.py` | Emulate C++ selection logic for debugging | ‚ùå No |

---

## ‚≠ê graphbrew_experiment.py - Main Orchestration (continued)

### Command-Line Options

See [[Command-Line-Reference]] for the complete CLI option reference.

Key flags: `--full` (complete pipeline), `--size small|medium|large`, `--phase reorder|benchmark|cache|weights|adaptive`, `--train` (multi-phase weight training), `--train-benchmarks` (default: pr bfs cc), `--train-iterative` (feedback loop), `--brute-force` (validation), `--auto` (auto-detect RAM/disk limits).

**Variant testing:** `--all-variants`, `--graphbrew-variants`, `--rabbit-variants`, `--gorder-variants`. Variant lists defined in `scripts/lib/core/utils.py`.

### Examples

```bash
python3 scripts/graphbrew_experiment.py --full --size small
python3 scripts/graphbrew_experiment.py --train --size small --max-graphs 5
python3 scripts/graphbrew_experiment.py --brute-force --validation-benchmark pr
python3 scripts/graphbrew_experiment.py --generate-maps --size small
```

---

## üì¶ lib/ Module Reference

The `lib/` folder is organised into five sub-packages. All public names are re-exported from `lib/__init__.py` for backward compatibility.

### core/ ‚Äî Constants, logging, data stores

| Module | Purpose | Key Exports |
|--------|---------|-------------|
| `utils.py` | **SSOT** constants & utilities | `ALGORITHMS`, `BENCHMARKS`, `BenchmarkResult`, variant lists, `canonical_algo_key()`, `run_command` |
| `graph_types.py` | Core types | `GraphInfo`, `AlgorithmConfig` |
| `datastore.py` | Unified data store | `BenchmarkStore`, `GraphPropsStore`, `adaptive_models.json` |
| `graph_data.py` | Per-graph storage | `GraphDataStore`, `list_graphs`, `list_runs` |

### pipeline/ ‚Äî Experiment execution stages

| Module | Purpose | Key Exports |
|--------|---------|-------------|
| `dependencies.py` | System deps | `check_dependencies`, `install_dependencies` |
| `build.py` | C++ build | `build_benchmarks`, `build_converter` |
| `download.py` | Graph download | `download_graphs`, `DOWNLOAD_GRAPHS_SMALL/MEDIUM` |
| `reorder.py` | Vertex reordering | `generate_reorderings`, `ReorderResult`, `load_label_maps_index` |
| `benchmark.py` | Benchmarking | `run_benchmark`, `run_benchmarks_multi_graph`, `run_benchmarks_with_variants` |
| `cache.py` | Cache simulation | `run_cache_simulations`, `CacheResult`, `get_cache_stats_summary` |
| `phases.py` | Phase orchestration | `PhaseConfig`, `run_reorder_phase`, `run_benchmark_phase`, `run_full_pipeline` |
| `progress.py` | Progress tracking | `ProgressTracker` (banners, phases, status) |

### ml/ ‚Äî ML scoring & training (legacy / fallback)

| Module | Purpose | Key Exports |
|--------|---------|-------------|
| `weights.py` | **SSO** scoring (fallback) | `PerceptronWeight`, `compute_weights_from_results`, `cross_validate_logo` |
| `eval_weights.py` | Data loading | `load_all_results`, `build_performance_matrix`, `compute_graph_features` |
| `training.py` | ML training | `train_adaptive_weights_iterative`, `train_adaptive_weights_large_scale` |
| `adaptive_emulator.py` | C++ emulation | `AdaptiveOrderEmulator` |
| `oracle.py` | Oracle analysis | `compute_oracle_accuracy`, `compute_regret` |
| `features.py` | Graph features | `compute_extended_features`, `detect_graph_type`, `get_available_memory_gb` |

### analysis/ ‚Äî Post-run analysis & visualisation

| Module | Purpose | Key Exports |
|--------|---------|-------------|
| `adaptive.py` | Adaptive analysis, A/B testing | `analyze_adaptive_order`, `parse_adaptive_output` |
| `metrics.py` | Amortization & E2E | `compute_amortization`, `format_amortization_table`, `AmortizationReport` |
| `figures.py` | Plot generation | SVG / PNG figures for wiki |

### tools/ ‚Äî Standalone CLI utilities

| Module | Purpose |
|--------|---------|
| `check_includes.py` | Scan C++ headers for legacy includes |
| `regen_features.py` | Regenerate graph features via C++ binary |

### Key: `compute_weights_from_results()`

The primary training function: multi-restart perceptrons (5√ó800 epochs, z-score normalized, L2 regularized) ‚Üí variant-level weight saving ‚Üí regret-aware grid search for benchmark multipliers ‚Üí stages to `type_0.json` (merged into `adaptive_models.json` by `export_unified_models()`). See [[Perceptron-Weights]] for details.

---

## üìè lib/analysis/metrics.py - Amortization & End-to-End Evaluation

**Post-hoc analysis of existing benchmark results ‚Äî no new benchmarks needed.**

Computes derived metrics from `benchmark_*.json` and `reorder_*.json`:
- **Amortization iterations** ‚Äî how many kernel runs to break even on reorder cost
- **E2E speedup at N iterations** ‚Äî speedup including amortized reorder cost
- **Head-to-head variant comparison** ‚Äî crossover points between two algorithms

### Quick Start

```python
from scripts.lib.analysis.metrics import compute_amortization_report, format_amortization_table
from scripts.lib.core.datastore import BenchmarkStore

store = BenchmarkStore()
benchmark_results = store.load_benchmark_results("results/benchmark_latest.json")
reorder_results = store.load_reorder_results("results/reorder_latest.json")

report = compute_amortization_report(benchmark_results, reorder_results)
print(format_amortization_table(report))
```

### Key Formula

`iters_to_amortize = reorder_cost / (baseline_time - reordered_time)`

### Verdict Categories

| Verdict | Amort Iters | Meaning |
|---------|:-----------:|---------|
| INSTANT | < 1 | Reorder cost negligible |
| FAST | 1‚Äì10 | Pays off quickly |
| OK | 10‚Äì100 | Worth it for repeated use |
| SLOW | > 100 | Only for many iterations |
| NEVER | ‚àû | Kernel is slower, never pays off |

### Output Columns

| Column | Description |
|--------|-------------|
| Kernel | Kernel-only speedup vs ORIGINAL |
| Reorder | Time spent computing the reordering |
| Amort | Iterations needed to break even |
| E2E@1 | End-to-end speedup at 1 iteration |
| E2E@10 | End-to-end speedup at 10 iterations |
| E2E@100 | End-to-end speedup at 100 iterations |
| Verdict | Human-readable break-even summary |

### Head-to-Head Comparison

The `--compare ALGO_A ALGO_B` flag produces a per-graph table showing:
- Which variant has faster kernels
- Which wins at E2E@1, @10, @100
- The **crossover iteration** where the slower-to-reorder variant overtakes
- Overall win/loss counts

The amortization report is also auto-printed by `graphbrew_experiment.py` after Phase 2 (benchmark) completes.

---

## Custom Pipeline Example

Create custom experiment pipelines using `lib/pipeline/phases.py`:

```python
#!/usr/bin/env python3
"""Custom GraphBrew pipeline example."""

import sys
sys.path.insert(0, "scripts")

from lib.pipeline.phases import PhaseConfig, run_reorder_phase, run_benchmark_phase
from lib.core.graph_types import GraphInfo
from lib.pipeline.progress import ProgressTracker

# Discover graphs
graphs = [
    GraphInfo(name="web-Stanford", path="graphs/web-Stanford/web-Stanford.mtx",
              size_mb=5.2, nodes=281903, edges=2312497)
]

# Select algorithms
algorithms = [0, 7, 12, 15]  # ORIGINAL, HUBCLUSTERDBG, GraphBrewOrder, LeidenOrder

# Create configuration
config = PhaseConfig(
    benchmarks=['pr', 'bfs'],
    trials=3,
    progress=ProgressTracker()
)

# Run phases
reorder_results, label_maps = run_reorder_phase(graphs, algorithms, config)
benchmark_results = run_benchmark_phase(graphs, algorithms, label_maps, config)

# Print results
for r in benchmark_results:
    if r.success:
        print(f"{r.graph} / {r.algorithm_name} / {r.benchmark}: {r.avg_time:.4f}s")
```

See the `lib/pipeline/phases.py` module for phase orchestration details.

---

## Output Structure

Results are organized as:
- `results/graphs/{name}/features.json` ‚Äî static graph features
- `results/logs/{name}/runs/{timestamp}/` ‚Äî per-run benchmarks, reorder, weights
- `results/mappings/{name}/` ‚Äî pre-generated label mappings (`.lo` + `.time`)
- `results/benchmark_*.json`, `reorder_*.json`, `cache_*.json` ‚Äî aggregate results
- `results/data/adaptive_models.json` ‚Äî trained model weights for C++

Use `python3 -m scripts.lib.core.graph_data --list-graphs` to browse per-graph data.

---

## Installation

```bash
cd scripts
pip install -r requirements.txt
```

### requirements.txt

```
# Core dependencies - standard library only for basic benchmarking
# These are REQUIRED for training, evaluation, and analysis scripts:
pytest>=7.0
networkx>=2.6
numpy>=1.20.0

# Optional: For extended analysis and visualization (uncomment if needed)
# pandas>=1.3.0        # For data manipulation  
# matplotlib>=3.4.0    # For plotting results
# scipy>=1.7.0         # For correlation analysis
```

---

## Troubleshooting

### Import Errors
```bash
pip install -r scripts/requirements.txt
python3 --version  # Should be 3.8+
```

### Binary Not Found
```bash
make all
make sim  # For cache simulation
```

### Permission Denied
```bash
chmod +x bench/bin/*
chmod +x bench/bin_sim/*
```

---

## Next Steps

- [[AdaptiveOrder-ML]] - ML perceptron details
- [[Running-Benchmarks]] - Command-line usage
- [[Code-Architecture]] - Codebase structure

---

[‚Üê Back to Home](Home) | [Code Architecture ‚Üí](Code-Architecture)
